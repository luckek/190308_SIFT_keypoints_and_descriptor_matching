{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT (using opencv)\n",
    "\n",
    "## Binary Image matching\n",
    "Through the development of your own image stitching utility, you've learned the utility of being able to match keypoints in images, and how the process is done (find a good keypoint, extract a local descriptor, match these descriptors against each other).  However, the keypoint detection and matching that we implemented ourselves was sub-optimal in the sense that it did not preserve important invariances like rotation and scale.  For our endeavours into 3D scene reconstruction, we will instead switch to the de facto gold standard of keypoint matching methods, known as Scale Invariant Feature Transform, or SIFT.  \n",
    "\n",
    "In essence, SIFT works just like Harris Corners, patch descriptors, and normalized SSE, with a few extra tweaks to make it better.  Instead of implementing SIFT ourselves, we will use the excellent implementation from OpenCV.  However, there is a small issue: SIFT is a non-free algorithm (it is free for academic use, but restricted from unlicensed commercial use).  As such it is not bundled with the standard OpenCV library.  Instead, it must be aquired separately.  However, this is made easy using pip (for those of you with linux or mac).  A simple\n",
    "\n",
    "pip install --user opencv-contrib-python==3.4.2.17\n",
    "\n",
    "will suffice to get both opencv and the non-free libraries associated with it. \n",
    "\n",
    "Once opencv is installed, it's straightforward to use with numpy.  The generation of SIFT keypoints and detectors is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I_1 = plt.imread('I_1.jpg')\n",
    "I_2 = plt.imread('I_2.jpg')\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.04,edgeThreshold=10,sigma=1.6)\n",
    "kp1,des1 = sift.detectAndCompute(I_1,None)\n",
    "kp2,des2 = sift.detectAndCompute(I_2,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's as simple as that.  Now, with the keypoints and descriptors generated, we can match them using a simple $n^2$ matching utility (note that there are faster matchers out there that provide approximate matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = cv2.BFMatcher()\n",
    "matches = matcher.knnMatch(des1,des2,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll want to do some simple quality control, using the ratio test.  The matches object above is a list of lists, containing the best and second best matches according to the SSE error metric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error for each match can be found using match.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches[0][0].distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code the ratio test.  Create a list of 'good' matches that pass the test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches = []\n",
    "for m,n in matches:\n",
    "    # Compute the ratio between best match m, and second best match n here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is complete, opencv offers you a mechanism for producing side-by-side keypoint match plots just like the ones you generated for Harris corners.  The syntax for this is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.drawMatchesKnn(I_1,kp1,I_2,kp2,good_matches,None,flags=2) \n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate such a composite image using pictures that you take today in class (2 of the many), and using the good matches that you computed above**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
